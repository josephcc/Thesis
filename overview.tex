\setul{3.6pt}{.1pt}

\setul{3.6pt}{.1pt}

Whether it is a consumer reading reviews to compare products, a learner reading different tutorial and forum posts, or a data scientist analyzing a large dataset, users are often faced with large quantities of unstructured information beyond an individual's capacity to process them fully. Typically, users reduce task uncertainty (unknown unknowns) by processing individual pieces of information in order to learn deep qualitative insights. This process of learning the unknown unknowns in the dataset allows users to iteratively refine their goals and interests, which can both potentially invalidate prior decisions and opens new research directions \cite{mar2006exp}. However, the cost of evaluating learned insights (known unknowns) under global context can be high, prohibiting users to evaluate their generalizability and whether they lead to high-yield information patches \cite{pirolli1999information}. For example, a consumer who encountered a product recommendation on one webpage may need to search across the Web and consider many other sources to figure out if it is worth adding it to their shortlist for deeper comparisons. As they read online reviews they often discover new criteria that fits their personal interests, but it can be difficult for them to figure out how well a new criteria can differentiate all the different products on their shortlist. Similarly, a data scientist who observed an interesting phenomenon on a subset of data also needed to spend a lot of effort in order to figure out whether it generalizes to the rest of the dataset \cite{charmaz2007grounded}.  Most existing approaches either focused on aggregation techniques of unstructured data (e.g., topic modeling, review summarization and aspect extraction) or interaction techniques for structured data (e.g., faceted navigation and multivariate visualizations), and do not support this process of bottom-up exploration and interpretation of unstructured online data. This thesis explores systems and interaction techniques that support exploring unstructured datasets by allowing users to both gain deep insights from each piece of information and at the same time evaluate such local phenomena under global context. I investigated this approach under the following two domains of crowdsourced sensemaking and individual online sensemaking.

\section{Global Context and Crowdsourced Sensemaking}

Crowdsourcing markets, such as Amazon Mechanical Turk, offer a new paradigm for on-demand data processing at scale, allowing researchers and machine learning practitioners who do not have the capacity or resources to process a dataset themselves and request a group of crowdworkers to process them. This typically involves the task requester who examines a small part of the data to design the microtask instructions, and divides the dataset into smaller chunks to be distributed across different crowdworkers. Traditionally, crowdsourcing had focused on collecting simple human judgements such as extracting contact information from a webpage \cite{franklin2011crowddb} or recognizing characters on an image that automatic optical character recognition (OCR) algorithms were unable to process \cite{von2008recaptcha}. However, many real-world sensemaking tasks are often more complex and interdependent, requiring each crowdworker who only received a small subset of data to evaluate them with a better understanding of the global context. 

Considering the task of organizing a collection of text snippets by their common themes. Since each crowdworker only saw an arbitrary subset of the entire dataset, it can be difficult for them to come up with categories that are coherent and comprehensive under the global context. For example, if all the items in the sample were closely related, a crowdworker might generate overly fine-grained categories; conversely, if all the items in the sample were dissimilar, the crowdworker might not be able to identify common patterns to generate useful categories. 

Further, even in classification tasks where a requester provided predefined categories to the crowdworkers, it can be prohibitively effortful for requesters to explore enough data in order to generate clear and comprehensive guidelines (i.e., label definitions) that can eliminate ambiguity in data that allowed for subjective interpretations by the crowdworkers. This is due to the fact that prior work has shown even in expert labeling scenarios where machine learning practitioners labeled all items in a dataset using predefined categories, their own mental definition of the categories will typically evolve throughout the process as they examine more items \cite{kulesza2014structured}. For example, a requester developing labeling guidelines for a seemingly simple task of labeling images as either about ``cats'' or not might not be aware of the long tail of edge cases in the dataset. These edge cases can then be interpreted differently by different crowdworkers, leading to inconsistent labels for images about cartoon cats or tigers. Fundamentally, the main challenges here are firstly each crowdworker can only see a small subset of data due to the scope of microtasks that typically range from a few seconds to a few minutes. Secondly, the requesters who relied on crowdsourcing to scale up their capability to process larger quantities of information also may not have enough global context to generate comprehensive guidelines for the crowdworkers. Failure to address these challenges can lead to incoherent structures and inconsistent labels.

In the first part of this dissertation, I explore novel crowdsourcing approaches for generating globally coherent structures. For this, I built two systems that introduced new framework for crowd categorization (clustering) and classification, respectively, that can provide better support for global context : 

\begin{itemize}
    \item \textbf{Alloy}: A novel crowdsourcing workflow that focuses on organizing a collection of textual snippets into globally coherent categories by combining crowdsourcing and computation (\Cref{chap:alloy}). Instead of showing a fixed subset of data to each crowdworkers, Alloy introduced a novel interaction technique that allowed each crowdworkers to repeatedly sample from the entire dataset until they build up a better understanding of the global context to generate coherent categories. 

    \item \textbf{Revolt}: A novel paradigm for collecting classification labels for training machine learning models (\Cref{chap:revolt}). Instead of requiring requesters to generate comprehensive guidelines beforehand, which can potentially require them to explore a large portion of the datasets, Revolt uses crowdworkers to identify ambiguous items in data and generate categories for post-hoc decisions made by the requesters.
\end{itemize}


The two systems were evaluated against state-of-the-art crowdsourcing and machine learning approaches on a wide variety types of data including web snippets extracted from Google search results, research paper abstracts, Web images that were a subset of ImageNet \cite{deng2009imagenet} and Webpage classification datasets from a prior work \cite{kulesza2014structured}. We found evidence that the proposed techniques can provide better global context, either to crowdworkers or the requeters, leading to more coherent structures and consistent labels.

The main focus of the Alloy system was to categorize web snippets extracted from webpages in a Google search results using queries such as how do I grow better tomatoes or What does a planet need to support life. To further investigate the usefulness of the Alloy structures for end-users, I built a third crowdsourcing system, \textbf{Knowledge Accelerator} (\Cref{chap:ka}), that synthesized the categories of web snippets into sections of overview articles. We were surprised to find that the Alloy structure led to crowd-generated articles that outperform top Google search results published by experts. These results revealed that end-users valued information that were synthesized from across many different online sources, especially when an authoritative information source was not available. This led to the second part of this dissertation that explores novel systems that support individuals when foraging and learning from online information, and evaluating discoverings from reading information pieces of information under the global context of many information sources.

\section{Global Context and Individual Online Sensemaking}

Whether planning a trip to a new city or deciding which product to purchase, consuming and foraging information online through exploratory search has become how people make sense of the world.  People now have instant access to an enormous online bazaar of information produced by experts and novices with different personal preferences, backgrounds, and assumptions. Consider purchasing a desk lamp at an office supply store versus online, and the differences in scale of available options and evidence. Amazon lists over 4,000 different options and up to thousands of reviews for each option; Google returns hundreds of ``best desk lamps'' listicles; and Reddit\footnote{\url{http://reddit.com}} lists thousands of discussions on desk lamps.  While this rich repository of diverse perspectives has the potentials to empower consumers and learners to explore and understand available options thoroughly and make better decisions \cite{de2015navigating}, the seemingly infinite number of options and evidence scattered across numerous information sources are often well beyond an individual's capacity to process them \cite{simon1971designing}. While existing research largely focused top-down approaches to support this process, such as presenting average review ratings, summarizing reviews \cite{hu2004mining,li2010structure} or making recommendations directly \cite{bobadilla2013recommender}, prior studies have instead shown that consumers often take a bottom-up approach of deeply examining each pieces of information to gain insights and gradually build up a personal understanding of the information space \cite{gan2012helpfulness,mudambi2010research}. 

One explanation for the bottom-up approach is that online evidence, such as reviews, can be messy, subjective, potentially biased and scattered across online sources \cite{hoch1986consumer, racherla2012perceived,zhang2012human,chen2015tripplanner}. This required users to both interpret each piece of evidence to determine how well it fits their personal context, as well as using multiple information sources in order to verify them \cite{Racherla:2012:PUO:2400774.2401599}. Another factor could be the exploratory, dynamic and opportunistic nature of online exploratory search \cite{marchionini2006exploratory} -- as users develop a personalized framework for comparing options, they might discover new criteria and iteratively refine their goals and preferences, potentially invalidates prior decisions and opens new research directions \cite{pirolli1999information,pirolli2005sensemaking}. Fundamentally, users have a need to deeply explore and interpret each piece of evidence to discover options and criteria that align with their own personal goals and needs, but the overwhelming amount of available evidence, options and information sources can be prohibitive for them to evaluate such local insights under the global context.

In the second half of this dissertation, I explore three novel systems and interaction techniques that can better support global context in this bottom-up qualitative process and scaffold users' online exploration and decision-making process:

\begin{itemize}
    \item \textbf{SearchLens}: an interactive restaurant review search interface that enabled users to explore reviews to discover and build up a set of structured queries (i.e., sets of weighted keywords) that reflected their different  nuanced interests to search for restaurants. This enabled users to maintain their evolving interests throughout exploration. Using the queries, the system generated personalized visual explanations for each restaurant in the search results, allowing users to interpret and explore new options based on their current interests. (\Cref{chap:searchlens})

    \item \textbf{Weaver}: a browser extension that enabled the browser to identify common entities (i.e., restaurants and destinations) mentioned across open tabs to support travel planning. Weaver's interface allowed users to both evaluate a new option they encountered on one webpage using evidence about it extracted across information sources, as well as allowed them to efficiently forage, accumulate and re-access evidence about different options across their browser tabs throughout the process. (\Cref{chap:weaver})

    \item \textbf{Mesh}: an interface that scaffolds users in exploring online evidence (reviews and webpages) about products and progressively builds up a comparison table that reflects their personalized criteria and evaluation of online evidence. Allowing them to both evaluate how useful a newly discovered criteria was for differentiating the options to prioritize their effort, as well as keeping track of their own interpretation and summarization of evidence as they explore. (\Cref{chap:mesh})
\end{itemize}


The three systems were evaluated using controlled lab studies and field deployment studies. I found evidence that by providing better global context across multiple information sources to individuals can lead to higher incentives for externalizing user interests (\Cref{chap:searchlens}), gather and accumulate evidence across information sources with lowered effort (\Cref{chap:weaver}) and discovering deeper insights from data (\Cref{chap:mesh}).

Two high level models for providing global context during individual online sensemaking  emerged from the benefits provided in the three systems for providing global context tasks. My first insight is that users can not confidently make decisions based on a single piece of evidence, whether it is a product recommendation or an important criteria mentioned in a review. In this case, providing them with a better global landscape of using other information sources as confirmation can lower both the interaction costs of cross-referencing and also the mental costs of evaluating many pieces of evidence. My second insights is that individuals often have evolving goals throughout the process, encountering both new criteria or interesting soft preferences as they explore more evidence. Allowing them to keep track of their changing interests and using them to interpret new and existing options can provide a scaffold that leads to better decision-making. 

\section{Thesis Statement and Overview}

In many sensemaking scenarios, users often face large quantities of unstructured data and take a bottom-up approach to explore them in order to gain deep insights from data. However, individuals with limited capacity often can not process all the data to see a complete landscape of information. With only a local view of the whole picture, users can risk generating incoherent structures when organizing data or be overwhelmed by the amount of choices and evidence leading to high interaction and mental effort. To investigate methods that can better support global context in such scenarios, I consider the following as the thesis of this dissertation:

\textbf{Using interaction and visualization techniques, we can dynamically provide global context that matches users' evolving intentions throughout their exploration of large unstructured datasets. Supporting this will allow users to gain deeper insights from data and make better decisions with lowered efforts.} Specifically, this dissertation first investigates this thesis in the domain of crowdsourced sensemaking where both the requesters and crowdworkers each only saw a small portion of data but needed to create globally coherent and consistent structures. This dissertation also investigates the domain of individual online sensemaking where the amount of available choices and evidence are often well beyond an individual's capacity to process them.  

Concretely, this thesis makes the following contributions:


\begin{enumerate}
    \item An interaction pattern of allowing users to explore  large quantities of information that supports qualitative knowledge discovery from individual pieces of information and evaluating them under the global context.

    \item Five novel system designs and interaction techniques that can better support this new model in two domains -- crowdsourced and individual online sensemaking.

    \item The implementation of the systems and extensive lab and field evaluation that investigated their costs and benefits to the users.
\end{enumerate}

